/**
 * Agent Orchestrator - Manages AI agent execution
 */

import type { AxonConfig, Bead, Skill } from '../../types';
import { BeadsError } from '../../utils/errors';
import { AxonLLMClient } from '../llm';

export interface ExecutionContext {
  bead: Bead;
  spec: string;
  skills: Skill[];
}

export interface ExecutionResult {
  success: boolean;
  artifacts: {
    files: string[];
    commits: string[];
  };
  tokensUsed: number;
  cost: number;
}

interface GeneratedFile {
  path: string;
  content: string;
}

export class AgentOrchestrator {
  private config: AxonConfig;
  private llm: AxonLLMClient;

  constructor(config: AxonConfig) {
    this.config = config;
    this.llm = new AxonLLMClient();
  }

  /**
   * Execute a bead with the appropriate agent
   */
  async execute(
    context: ExecutionContext,
    onProgress?: (message: string) => void,
  ): Promise<ExecutionResult> {
    const { bead } = context;
    const agentConfig = this.config.agents[bead.agent] || this.config.agents.sisyphus;

    // Build prompt
    const prompt = this.buildPrompt(context);

    if (onProgress) {
      onProgress('ğŸ§  AI æ­£åœ¨æ„æ€å®ç°æ–¹æ¡ˆ...');
    }

    // Call LLM with streaming for progress feedback
    let fullContent = '';
    let tokensUsed = 0;
    let cost = 0;
    let lastProgressUpdate = Date.now();
    let chunkCount = 0;

    const iterator = this.llm.streamChat([{ role: 'user', content: prompt }], {
      agent: bead.agent,
      model: agentConfig.model,
      temperature: agentConfig.temperature,
      maxTokens: agentConfig.max_tokens,
    });

    let result = await iterator.next();
    while (!result.done) {
      const chunk = result.value;
      fullContent += chunk;
      chunkCount++;

      // Update progress every 500ms or 50 chunks
      if (onProgress && (Date.now() - lastProgressUpdate > 500 || chunkCount % 50 === 0)) {
        const wordCount = fullContent.split(/\s+/).length;
        onProgress(`ğŸ§  AI æ­£åœ¨ç”Ÿæˆä»£ç  (${wordCount} è¯)...`);
        lastProgressUpdate = Date.now();
      }

      result = await iterator.next();
    }

    const response = result.value;
    tokensUsed = response.tokens.input + response.tokens.output;
    cost = response.cost;

    if (onProgress) {
      onProgress('ğŸ“‚ æ­£åœ¨å¤„ç†ç”Ÿæˆçš„æ–‡ä»¶...');
    }

    // Parse and write files
    const artifacts = await this.processResponse(fullContent);

    return {
      success: true,
      artifacts,
      tokensUsed,
      cost,
    };
  }

  private buildPrompt(context: ExecutionContext): string {
    const { bead, spec, skills } = context;

    let prompt = `ä½ æ­£åœ¨æ‰§è¡Œ Axon ä»»åŠ¡å›¾ä¸­çš„ä»»åŠ¡ã€‚

**ä»»åŠ¡ä¿¡æ¯:**
- ID: ${bead.id}
- æ ‡é¢˜: ${bead.title}
- æè¿°: ${bead.description}
- ä¼˜å…ˆçº§: ${bead.priority}
- éœ€è¦æŠ€èƒ½: ${bead.skills_required.join(', ') || 'æ— '}

**é¡¹ç›®è§„æ ¼ (OpenSpec):**
${spec || '(æ— è§„æ ¼æ–‡æ¡£)'}
`;

    if (skills.length > 0) {
      prompt += '\n**å¯ç”¨æŠ€èƒ½æ¨¡æ¿:**\n';
      for (const skill of skills) {
        prompt += `
### ${skill.metadata.name}
${skill.content}
`;
      }
    }

    prompt += `

**è¦æ±‚:**
1. ä¸¥æ ¼æŒ‰ç…§ OpenSpec ä¸­çš„æ¶æ„å†³ç­–å®ç°
2. ä¼˜å…ˆä½¿ç”¨æä¾›çš„æŠ€èƒ½æ¨¡æ¿
3. ç”Ÿæˆçš„ä»£ç å¿…é¡»åŒ…å«å•å…ƒæµ‹è¯•
4. å®Œæˆåè¾“å‡º "DONE: ${bead.id}"

**è¾“å‡ºæ ¼å¼:**
è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºï¼š

\`\`\`json
{
  "files": [
    {
      "path": "src/example.ts",
      "content": "// æ–‡ä»¶å†…å®¹..."
    }
  ],
  "tests": [
    {
      "path": "tests/example.test.ts",
      "content": "// æµ‹è¯•å†…å®¹..."
    }
  ],
  "explanation": "å®ç°è¯´æ˜..."
}
\`\`\`
`;

    return prompt;
  }

  private async processResponse(text: string): Promise<{ files: string[]; commits: string[] }> {
    // Extract JSON from response
    const jsonMatch = text.match(/```json\n([\s\S]*?)\n```/);
    if (!jsonMatch) {
      throw new BeadsError('æ— æ³•è§£æ AI å“åº”: æœªæ‰¾åˆ° JSON å—');
    }

    let data: { files?: GeneratedFile[]; tests?: GeneratedFile[]; explanation?: string };
    try {
      data = JSON.parse(jsonMatch[1]);
    } catch {
      throw new BeadsError('æ— æ³•è§£æ AI å“åº”: JSON æ ¼å¼æ— æ•ˆ');
    }

    const files: string[] = [];

    // Write generated files
    for (const file of data.files || []) {
      await Bun.write(file.path, file.content);
      files.push(file.path);
    }

    // Write test files
    for (const test of data.tests || []) {
      await Bun.write(test.path, test.content);
      files.push(test.path);
    }

    return {
      files,
      commits: [],
    };
  }
}
